# SiteSpeak: Unified Voice-First Website Builder Architecture

Vision and Architecture Overview

SiteSpeak is a Wix/GoDaddy-class no-code website builder where every published site includes a built-in voice-first, agentic assistant that can understand the site’s content, perform actions (navigation, filtering, adding to cart, booking, etc.), and continuously update its own knowledge base by recrawling the site. In essence, each generated website is self-describing and effectively doubles as an API for the voice agent via a standard “Site Contract” that exposes the site’s structure and actions. This unified approach means the AI assistant is tightly integrated with the site’s content and capabilities from the moment of publishing.

Architecturally, SiteSpeak is implemented as a modular monolith following modern design principles (a hexagonal architecture and 12-Factor app methodology). All configuration is externalized, and the system cleanly separates concerns: for example, the web-serving processes (HTTP APIs, real-time voice server) are distinct from background worker processes (crawling and indexing jobs). This separation (enabled by containerization and a shared codebase) ensures scalability and maintainability – each service or module focuses on its role while interacting through well-defined contracts. Cross-cutting concerns (config management, logging, security, etc.) are centralized in a shared library to enforce consistency across the platform. The technology stack is TypeScript throughout (React on the frontend and Node.js/Express on the backend), with PostgreSQL and Redis for storage and caching, and OpenAI’s GPT-4 (via API) as the core language model driving the assistant. All components adhere to strict type-safety, clear naming conventions, and a domain-driven folder structure (e.g. services/voice, services/ai, services/analytics, services/_shared for common utilities). This coherent, single-repository architecture (as opposed to many microservices) reduces integration overhead but still respects domain boundaries through a plugin-like internal structure.

Key Architectural Tenets: The design emphasizes high performance, security, and standards compliance from the ground up. First, low latency is a non-negotiable goal – initial voice feedback should be near-instant (on the order of a few hundred milliseconds). Second, every site’s content and functionality are exposed in a machine-readable format (structured data, semantic markup, and a formal action schema) so that the AI agent can reliably parse and interact with the site. Third, the system is multi-tenant by design: each customer’s site data is isolated and protected, and the platform can scale to many sites while maintaining data privacy. Finally, the architecture is extensible – new tools or site features can be integrated by updating the Site Contract and tool registry without breaking the overall framework, thanks to strongly typed interfaces and a stable versioned API surface.

Below, we detail each major subsystem of SiteSpeak – from the front-end site builder and publishing pipeline, to the real-time voice interaction system, the knowledge base and indexing engine, the AI orchestration layer with its tool plugins, and the supporting infrastructure (API gateway, security, analytics, and deployment). Throughout, we highlight best practices (LangGraph agent orchestration, pgvector for embeddings, JSON-LD and ARIA for SEO/accessibility, optimistic UI updates, etc.) and how they are applied in our implementation.

Frontend Builder and Publishing Pipeline

No-Code Site Builder: SiteSpeak provides a drag-and-drop builder UI (web application) that allows users to design pages, add content (text, images, products, forms, etc.), and define navigation structure without coding. The builder uses a library of pre-built components (implemented in React/Tailwind with accessible Radix UI primitives) that are voice-enabled by default. Each component in the library carries metadata for the agent: for example, a Product Listing component might embed JSON-LD product schema and define an action for “Add to Cart.” Users can also configure global settings like their domain, theme, and any e-commerce or booking options through the builder’s interface.

When a user publishes a site, the system generates a static deployable package of the site and a machine-readable Site Contract. The Site Contract is a collection of artifacts that describe the site for both search engines and the SiteSpeak agent. Specifically, the publishing service emits:

A sitemap.xml listing all public pages with last modified timestamps. This includes accurate `<lastmod>` values for each URL so that both external crawlers and our internal system know when a page was last updated.

Structured data embedded in each page (primarily as JSON-LD in `<script>` tags) detailing key entities on the page (products, FAQs, blog posts, events, etc., following Schema.org vocabulary). This ensures that factual content is explicitly captured in a format that both Google and our agent understand. For example, if the site has a product catalog, each product page includes a Product schema with price, availability, and other details.

Semantic HTML and ARIA landmarks throughout the page markup. The builder wraps regions in elements like `<main>`, `<nav>`, `<header>`, `<footer>` (with appropriate ARIA roles) to delineate the page structure. Important interactive elements have recognizable roles and labels. This not only improves accessibility (WCAG compliance) but also gives the voice assistant clear hooks for locating content sections (e.g. “the footer”, “the navigation menu”) and interactive controls.

An Action Manifest (actions.json), which is a structured list of all interactive actions or functions the site offers. This includes things like navigational actions (e.g., a “Go to Contact page” link), form submissions (e.g., “Submit Newsletter Signup form”), e-commerce actions (e.g., “Add item X to cart”), and any custom widget actions. Each action is described with a unique name/ID, a CSS selector or target that the action corresponds to, a description, and a schema for any parameters it requires (for instance, a quantity for adding to cart, or form field inputs). The manifest acts as a bridge between the site’s front-end and the AI agent – essentially an API specification for what the agent can do on the site. It is generated automatically by scanning the site’s components and configuration (ensuring determinism and consistency across publishes).

Optionally, a GraphQL schema (or REST API description) if the site has dynamic data back-end. SiteSpeak can expose a /graphql endpoint on published sites with introspection enabled, describing collections like Products, Orders, BlogPosts, etc., if the user’s plan or configuration includes those features. This allows the voice agent to query the site’s data directly in a structured way. For example, instead of scraping an FAQ page, the agent could run a GraphQL query for FAQs. The GraphQL introspection results (types, fields, etc.) can be incorporated into the Action Manifest or tool definitions so the agent knows these queries are available. This is part of making each site fully machine-readable out of the box.

Once these artifacts are generated, the publishing pipeline takes the static site (HTML/CSS/JS and assets) and deploys it to the hosting infrastructure. SiteSpeak uses content-addressed deploys for efficiency and cacheability: all asset files (CSS, JS bundles, images) are named with hashes or version identifiers. This allows long CDN cache lifetimes and ensures atomic deployment – new files won’t conflict with old ones because their names differ if content changes. The publishing service performs an atomic upload of the new version and then updates a pointer (such as a CDN configuration or database entry) so that the custom domain or site URL now serves the new version. This is akin to a blue/green deployment: the old version remains serving until the switch, and rollback is as simple as repointing to the previous version if needed. During the cutover, the cache control headers are set such that browsers will fetch updated content (HTML is usually not cached, or very short TTL, while hashed assets have a far-future TTL). The result is zero downtime and no inconsistent state (the entire site updates in one go).

After publishing, the system triggers a knowledge base indexing job (full or incremental) for the site (see next section). The publish event includes a diff of what changed, so the crawler knows whether to recrawl all pages or just some. Site owners can publish changes multiple times; each publish creates a new version, and older versions can be archived or used for rollback if necessary.

Indexability and SEO: The published sites are not only optimized for the voice agent but also follow SEO best practices. The JSON-LD structured data and sitemap help search engines index the content effectively (e.g., products are eligible for rich results). Additionally, the site’s performance is optimized (lazy loading images, using CDNs, and meeting Core Web Vitals) to ensure good page experience rankings. The voice assistant features do not impede normal site use – they load as an overlay and are implemented in a way that doesn’t hurt page load metrics significantly (the widget script is optimized and can be deferred until after core content loads).

In summary, the builder and publishing subsystem ensures that whenever a user’s site goes live, it is voice-ready, agent-readable, and web-optimized. The site contract (sitemap, JSON-LD, ARIA, actions) acts as a formal specification of the site’s content and capabilities, which the rest of the SiteSpeak system will use to provide the intelligent voice layer.

Knowledge Base Ingestion and Indexing

Once a site is published, SiteSpeak’s knowledge base (KB) service takes over to ingest the site’s content and create a private, queryable index for the voice assistant. The goal is to maintain a complete, up-to-date representation of the site in a form that can be efficiently searched and referenced during a conversation. This knowledge base is essentially the site’s “brain,” stored server-side.

Crawling Pipeline: A dedicated crawler (which can run in headless Chrome via Playwright for accuracy) reads the Site Contract and begins fetching the site’s pages. It starts with the sitemap.xml to get the list of URLs and their last modified dates. The crawler is polite and standards-compliant: it respects robots.txt rules (though by default, sites built with SiteSpeak are expected to allow their own agent) and uses the `<lastmod>` timestamps from the sitemap as hints to avoid unnecessary fetching. For each page, it issues conditional GET requests (using ETag or If-Modified-Since headers) and will skip downloading content if the page hasn’t changed. When a page is fetched, a content hash is computed (e.g., a hash of the cleaned text plus relevant structured data) to quickly detect changes on future crawls and to avoid re-processing identical content. Only pages that are new or have changed since the last index are fully processed – this delta indexing strategy prevents redundant work and keeps indexing efficient.

The crawler runs in a continuous or scheduled job mode. In one scenario, a full crawl happens at initial publish (or when a site is first added), and then incremental updates happen periodically (say every N minutes or hours) or when triggered by events (like the site owner clicking “Reindex” or a webhook indicating content updates). A site-specific queue ensures we don’t crawl the same site concurrently in multiple jobs, and within a site’s crawl, we avoid hitting the same page with overlapping requests. Backoff strategies (exponential delay on errors) and concurrency limits (e.g. only 1–2 pages fetch simultaneously per site) protect the target site and ensure we operate within resource budgets.

Extraction and Chunking: For each fetched page, the raw HTML is passed through a series of extractors and transformers to pull out the meaningful pieces for the knowledge base:

First, any embedded JSON-LD or structured data is parsed (this is low-hanging fruit for factual info). For example, if a page has a Product schema or FAQ schema, those facts (like “price: $19.99” or Q&A pairs) are extracted as structured triples or text snippets. These become high-quality knowledge chunks (often treated specially since they’re canonical facts). Structured data is preferred as it’s already clean and normalized.

Second, the visible text content of the page is extracted from the HTML. The extractor (often using something like Readability or custom parsing) will pull out paragraphs, headings, list items – essentially all user-facing textual information – while stripping boilerplate (navigation menus, footers, etc., which are common to many pages). It retains the semantic context: for each text chunk, we record its heading path (h1/h2 hierarchy), the page URL, and maybe its DOM location or ARIA region (e.g., “this paragraph is in the Main content area > Section: About Us”). This context is stored as metadata.

Third, the crawler identifies interactive elements and forms on the page. Using the Action Manifest and also heuristic scanning (like looking for data-action attributes or form tags), it captures details about actions (clickable buttons, links that do something, etc.) and forms (input fields and their labels, validation rules, and form submission endpoints). For example, if there’s a “Contact Us” form, the knowledge base will have an entry describing that form (what fields it has, what the action URL is). If there’s an “Add to Cart” button with a data-action="product.addToCart", the KB knows that action and can link it to the product data.

The content is then chunked into pieces suitable for embedding. We aim for chunk sizes that are neither too large nor too small – for instance, targeting ~200-500 tokens of text per chunk (perhaps a paragraph or a Q&A pair). Chunk boundaries respect semantic structure (don’t cut off in middle of a sentence, etc.), and each chunk inherits the metadata of where it came from (page URL, section, etc.). For long pages, overlapping windows may be used with slight overlap to ensure continuity, but overlap is kept minimal (10-20%).

After extraction, we have various artifact tables: e.g., kb_documents (one entry per page/URL with high-level info like last crawled time and page hash), kb_chunks (many entries, each with a piece of text or a fact and references to its document), kb_actions (the interactive actions found, like buttons or API calls, often pre-filled from the Action Manifest), kb_forms (form definitions), and potentially kb_entities for structured entities. These are all tagged by tenant and site IDs to enforce isolation.

Embedding and Indexing: Each text chunk (and possibly some structured data items) is then turned into a vector embedding using a language model (OpenAI’s text-embedding model) for semantic search. We use the default model (e.g., text-embedding-3-small, 1536-dimensional) unless a higher precision is needed, in which case a larger model can be configured. The embeddings are stored in a PostgreSQL table with the pgvector extension enabled. We create a vector index (either HNSW or IVFFlat) on these embeddings to allow fast approximate nearest neighbor searches. In practice, HNSW is chosen for most use-cases because it provides low-latency lookups suitable for real-time queries (critical for voice), especially when the corpus per site is moderate (HNSW has excellent recall and speed at the cost of some memory). For very large sites with massive content, IVFFlat indexes could be considered to save memory, but they might come with a slight recall penalty – we document those trade-offs and choose per tenant if needed.

In addition to vector search, the knowledge base supports hybrid retrieval: we store a full-text search index (Postgres tsvector) of the content as well. When the assistant queries the KB, we can combine semantic similarity scores with keyword matching scores to ensure precise terms (like product IDs or names) aren’t overlooked. A rank fusion technique (e.g., Reciprocal Rank Fusion) is used to merge the results from the vector index and the FTS index for a more robust retrieval. We also have the capability to re-rank top results using a heavier model (like a cross-encoder) if needed for quality, though under tight latency budgets this may be skipped or done offline.

All pieces of content in the KB are linked via stable IDs and relations. For example, if an action “Add to Cart” is available on a product page, the KB entry for that action will reference the product or page it’s on. This way, a search result can bring not just a text snippet but also affordances – e.g., “This paragraph is about Product X, and here’s an action to add that product to cart”. The voice assistant can use those affordances immediately (more on that in the orchestration section).

Closed-Loop and Isolation: Each site’s knowledge base is completely self-contained (“closed-loop”) – it knows everything about that site, and nothing about any other site. We enforce multi-tenancy at the database level using either separate schema per tenant or row-level security policies, so queries for one site cannot ever return data for another. The system’s design explicitly forbids any cross-tenant data mix, guaranteeing that one customer’s site data (and user queries) remain isolated. This is both a privacy feature and to ensure relevance (you don’t want the agent answering from someone else’s content). In practice, each query to the KB is always scoped with a tenant/site identifier as part of the query or embedding lookup key.

Knowledge Update Triggers: The KB is kept up-to-date through several mechanisms. On every new publish, as mentioned, a full or partial re-index is triggered (often partial, since we know which pages changed). Additionally, a periodic job may poll the sitemap for changes or simply re-run the extraction on a schedule (say, nightly) to pick up any changes not resulting from a publish (for instance, if the site pulls in external data that changes). Site owners can also manually request a re-index via the SiteSpeak dashboard (using the API endpoint, which the gateway exposes). Because indexing is efficient and idempotent – unchanged content is skipped – these re-index operations can be run fairly frequently.

Moreover, any time the site content changes in real time (for example, an e-commerce inventory count or a new blog post via an API), the site could call a webhook or otherwise signal SiteSpeak. In Phase 2, we envision a more event-driven update: a content publish event or CMS update triggers an indexer job for just that item (using the outbox/event pattern in our shared events system). This keeps the KB “eventually consistent” with the live site with minimal delay.

Storage and Scale: The knowledge base is stored in PostgreSQL, which serves as both the primary relational store and the vector store (via pgvector). By using Postgres, we benefit from strong consistency and the ability to join vector search results with metadata (filters by category, etc.) easily. For scalability, we can partition the data by tenant or time if needed, and use read replicas for heavy read loads. Given that each site’s content is relatively modest (most small-business sites have perhaps tens or hundreds of pages, not millions), this design is more than sufficient. Should a particular tenant have a huge amount of content, we could consider sharding their data or moving their vectors to a dedicated service, but that’s an edge case.

In summary, the knowledge base subsystem ingests content automatically from every SiteSpeak site and keeps it fresh. It ensures that the voice assistant can retrieve factual information and context quickly (in ~10s of milliseconds for vector lookup) and reliably, using a combination of semantic and lexical search. The KB treats the site as the source of truth: it never invents information beyond what’s on the site. And because it’s always updating after changes, the assistant’s answers remain up-to-date (a key advantage over static site chatbots). This subsystem, combined with the structured Site Contract, means there’s no manual work needed to “train” the AI on the site – it’s all automated and deterministic.

Real-Time Voice Interaction System

One of the crown jewels of SiteSpeak is the real-time voice assistant that users (site visitors) can interact with on every published site. This subsystem spans the front-end voice widget in the browser and the backend streaming voice service. The design goals are to make voice interaction fast, natural, and non-intrusive. The user should feel like they are having a fluid conversation with the website – asking questions or giving commands and getting spoken responses or actions done in real time.

Voice Widget (Client-side): Every SiteSpeak-published site includes a small embedded JavaScript widget that injects the voice assistant UI into the site. Typically, this appears as a floating microphone button in the corner of the page (with a tooltip like “Talk to us” or “Ask our AI Assistant”). The widget is implemented with accessibility in mind: it can be focused via keyboard, has appropriate ARIA labels, and does not obstruct other content. When the user activates it, it opens a chat-like panel showing a live transcription of the user’s speech and the assistant’s responses, as well as any relevant visual highlights or suggestions.

Under the hood, clicking the mic button will use the Web Audio API to access the device’s microphone (the user is prompted by the browser for permission). We use an AudioWorklet for low-latency audio processing on the client. This worklet captures audio samples from the mic in a separate audio thread, enabling us to perform real-time operations like voice activity detection (VAD) without blocking the main UI thread. We enable microphone DSP features like echo cancellation, noise suppression, and auto gain control when requesting audio, to improve quality. The audio is then encoded into Opus frames (20ms frames at 48 kHz mono) in real time. Opus is chosen as the codec for its low latency and efficiency in transmitting speech.

The widget establishes a full-duplex connection to the SiteSpeak backend – this can be either a WebSocket connection or a Server-Sent Events (SSE) connection, depending on environment (WebSocket is preferred for truly bidirectional needs). Over this connection, the client will stream the Opus audio frames as the user speaks, and simultaneously receive events from the server (like partial transcripts and the AI’s response text or audio). We implement a custom lightweight protocol on this WebSocket: audio is sent as binary messages (tagged with an audio content type), and textual events from the server (like partial_asr (partial speech-to-text result), final_asr, agent_response_delta, or action_execute) come as JSON messages. The WebSocket server on the backend is tuned for real-time: it handles pings/pongs to keep the connection alive and measures latency, and it monitors the client’s send buffer to apply backpressure if needed (e.g., if network is slow, we may drop some non-critical messages like intermediate VAD signals to catch up).

Streaming Speech-to-Text (ASR): As soon as the user begins speaking, the backend starts streaming the audio to an ASR service. SiteSpeak leverages the OpenAI Whisper Realtime API (via our proxy) or a similar streaming speech-to-text service. This means the audio is transcribed incrementally: within ~100-150ms of the user speaking a word, a partial transcript of that word (or sounds of it) is generated. The backend pushes these partial transcripts back to the client in a steady stream, so the user can see live captions of what they’re saying. This feedback is important for user confidence and also accessibility (it doubles as live captions). The voice widget displays the partial transcript in a subtle gray italic text in the chat bubble, which updates in real-time (using aria-live="polite" so screen readers can announce updates without being jarring).

Once the user finishes speaking (the VAD in the AudioWorklet detects silence for a certain hang period), the final ASR result is sent. The backend provides the final text along with language detection (so we know if the user spoke in Turkish vs English, for instance). The client then shows the final transcript in normal font weight (converting the gray partial text to solid text).

Throughout, the latency of this ASR pipeline is extremely low – partial words come nearly as they are spoken, and the final transcript arrives typically within a second (depending on utterance length). Our target is to have the first AI response token ready by the time the user has finished their sentence or very shortly after, which requires overlapping the ASR and understanding steps with the tail end of the user’s speech.

Real-Time NLU and Agent Orchestration: As soon as the backend gets the first bits of the user’s speech transcribed, the orchestrator AI (described in the next section) is invoked to start formulating a response or action plan. Because we use a streaming architecture, we don’t wait for the entire user utterance to finish to begin processing – in some cases, we can predict or start searching the knowledge base before the user has completed their question. For example, if the user says "Do you have _", the system might start looking up product inventory in anticipation. However, the core plan is usually formulated once the full utterance is available (to avoid misfires).

The orchestrator (which uses GPT-4 or similar) takes the user’s query, retrieves relevant knowledge base snippets (if it’s a question) and decides if any tools (functions) need to be executed (details in next section). All of this planning happens typically within a few hundred milliseconds. The result is an agent “plan” that might include an answer to speak and/or an action to take on the site.

Streaming Text-to-Speech (TTS): As the agent’s response is generated, we also produce audio to speak back to the user. We again use a streaming approach: leveraging a TTS service that can stream audio chunks for the output text (OpenAI’s real-time endpoint can do this, or other providers like Azure Cognitive Services with neural voices). The backend begins sending down audio chunks (Opus-encoded or raw PCM) over the WebSocket as soon as the TTS starts producing them. On the client side, these chunks are received and immediately fed to an audio output buffer. We either use the Web Audio API to decode and play the Opus audio or, if the service returns a playable format like a WAV or MP3 stream, we append it to a MediaSource for continuous play. This means the user hears the assistant start speaking the answer in near real-time, even if the full answer is not ready yet. For longer answers, the speech will continue to stream out chunk by chunk.

Because of this streaming, the first spoken word of the assistant often comes out within ~300ms from when the user finished speaking (assuming network and processing are optimal). This is a critical benchmark for us – it creates the impression of an almost instantaneous response. In fact, this ≤300ms first-token latency is a hard requirement we measure and optimize for (e.g., using partial planning, caching frequent Q&A, etc.). The system even employs speculative techniques (like prefetching likely data or using the browser’s `<link rel="prefetch">` hints) to achieve this responsiveness.

Barge-in and Interruptions: A hallmark of a good voice system is support for barge-in, meaning the user can interrupt the assistant’s speech with their own voice, and the system will stop speaking and listen. SiteSpeak handles this robustly. The client’s AudioWorklet continuously runs VAD even while the assistant is speaking. If it detects the user started talking over the assistant (voice activity while TTS audio is playing), it immediately emits a barge_in event. The client then pauses or ducks the TTS playback within ~50ms, so the user doesn’t have to fight to be heard. The moment this happens, a new recording stream begins (we either reuse the same WebSocket connection by sending a special message to reset the ASR stream, or open a new channel if needed). The assistant’s prior response is effectively cut off. On the backend, the orchestrator is notified that the user interjected, and it will abandon whatever it was doing for the old query (if it’s still mid-process) – because there’s a new user query coming in. This way, the dialogue can be rapid and feel natural, with the user able to ask follow-ups or clarifications without waiting for the assistant to finish an answer they don’t need. Our implementation ensures barge-in triggers both on the client (for UI/UX) and server (so the AI stops generating) are synchronized and fast.

Visual and Interactive Feedback: While voice is the primary mode, the system provides rich visual feedback to augment the conversation. The voice widget interface is designed to be minimal but informative, following the principle of calm design – showing only what is necessary to build trust in the system’s status. Key elements of the UI feedback include:

A mic indicator and volume meter: When listening, the mic button usually glows and a small waveform or level bar animates to show the volume of detected speech. This reassures the user that the system is “hearing” them. The level is driven by our VAD’s signal energy.

Partial transcript text: As mentioned, what the system thinks the user is saying is shown in real-time. This text is typically gray and italic until finalized.

“Thinking” indicator: As the agent (LLM) is formulating an answer, the widget might show a subtle loader or animated ellipsis to indicate the assistant is working. However, since we strive to start speaking quickly, often this is almost instantly replaced by the actual response text.

Tool action highlights: If the assistant triggers a tool that affects the page, the relevant part of the page is highlighted to the user. For example, if the assistant says "Okay, let me check that for you" and in the background it navigates to a different page or opens a menu, the UI might momentarily highlight that menu or flash an outline around a product that it’s talking about. This is achieved by the front-end receiving a message that a certain action (identified by a CSS selector or element) was executed, and then applying a brief CSS animation (like a colored outline fading) on that element.

Error messages: If something goes wrong (e.g., network drops, or ASR fails), the widget will display a friendly error message in the chat pane (and possibly an ephemeral toast). For instance, “Oops, I didn’t catch that. Please try again.” for ASR issues, or “Having trouble connecting, one moment…” for network issues. These messages are important to set user expectations when things fail. They are also spoken via TTS if appropriate, or a sound cue can be played for errors.

Text input fallback: While voice is primary, the widget typically also allows the user to type their question if they prefer (especially if they are in a noisy environment or cannot use sound). There’s usually a text input field in the widget panel for this. Typed queries go through the same backend, but obviously skip the ASR and TTS parts.

From a performance standpoint, the voice system is optimized to meet stringent latency targets. We aim for median partial transcription latency ≤150ms and barge-in response (TTS pause) within 50ms of speech detection. The entire pipeline from the end of user speech to begin of system speech is targeted at ~300ms or better. In practice, achieving this requires careful engineering at every layer: the use of binary audio frames for minimal overhead, maintaining persistent warm connections to the ASR/TTS providers, using efficient codecs, and possibly running some language processing on the edge (or ahead-of-time caching responses to very common queries). Our measurements show that for short queries, we can often beat the 300ms mark for first word. For longer or more complex tasks (that may involve database lookups or external API calls through tools), the system will at least speak some filler or acknowledgment quickly (like “Let’s see...”) to avoid dead air, and then continue with the actual answer as soon as it’s ready (this strategy of immediate feedback is sometimes called an optimistic prompt or conversational latency hiding).

Internationalization: The voice system supports multiple languages. The default ASR and TTS locales are chosen based on the site or user’s preference. We parse the browser’s Accept-Language header to set a default locale (e.g., if the site visitor’s browser is in Turkish, we default to Turkish for voice). This locale is communicated to the ASR/TTS services. The user or site owner can override it (for instance, a site might default to English regardless of browser, or provide a language switch UI). The assistant’s NLU (LLM) can handle multilingual input, but we also localize any tool outputs or content where needed. We ensure that when the assistant speaks, it uses an appropriate voice (we might choose a neural voice that matches the language/gender/tone desired by the site owner). All messages and errors in the UI are localized as well.

In summary, the real-time voice UX is designed to be instant, interruptible, and intuitive. By streaming both ASR and TTS and providing immediate visual feedback, the user is kept in the loop at every moment – they see the assistant “listening” and “thinking,” they hear it respond almost immediately, and they can always interrupt if it’s not going in the direction they want. The voice interface turns passive websites into interactive, conversational experiences while respecting the user’s time and attention. All of this is achieved with a careful orchestration of client-side and server-side technologies working in concert.

AI Assistant Orchestration and Tools (LangGraph)

At the core of SiteSpeak’s intelligent behavior is the AI Assistant Orchestrator, a system that interprets user requests (from voice or text) and determines how to fulfill them. This orchestrator is essentially a custom chain-of-thought agent powered by an LLM (like GPT-4), enhanced with a suite of domain-specific tools it can call, and managed by a graph-based workflow engine (we use LangGraph, a TypeScript library for building agent flows, integrated with LangChain function calling). The design here centers on making the assistant actionable – not just answering questions with text, but actually performing tasks on the user’s behalf using the site’s capabilities.

Tool Registry and Schema: We maintain a central Tool Registry that defines all the functions (actions) the AI can invoke. Each tool has a name, description, input schema, and an execution handler. We use Zod to define the schema for each tool’s parameters, and we automatically export those as JSON Schema (which is compatible with OpenAI function calling format). This means the LLM is provided with a machine-readable spec of what each function does and what arguments it expects. Tools also carry metadata such as whether they have side effects, require user confirmation, or have an expected latency budget.

SiteSpeak includes several built-in tool modules, corresponding to common website functionalities:

Navigation Tools (navigation.ts): e.g., goto(page) to navigate to another page/URL on the site, scrollTo(target) to scroll an element into view, highlight(selector) to highlight an element. These are marked as having no data side-effects (just UI navigation) and are allowed to execute optimistically without confirmation. In fact, when the agent is quite sure (high confidence) that the user wants to go to a certain page, it can call goto immediately – the front-end will begin navigation even while the agent is still formulating the rest of its answer, making the interaction feel snappy. For instance, if the user says "Take me to the Pricing page", the assistant can instantly trigger the page change.

Search Tools (search.ts): e.g., siteSearch(query, filters) which performs a search in the site’s knowledge base (and optionally also queries a site’s internal search API if it exists). This allows the agent to retrieve information it doesn’t already have in context. Another could be suggestNext(context) which returns some suggested next questions or related pages (helpful for guiding the user).

E-commerce Tools (commerce.ts): If the site has e-commerce features, tools like addToCart(productId, quantity) to add an item to the cart, listVariants(productId) to retrieve available variants (sizes, colors), applyCoupon(code), startCheckout(cartId) to initiate a checkout process, and placeOrder(orderId) to finalize an order. These tools often have side effects (cart or order database changes) and some are marked as requiring confirmation – for example, placeOrder would be set confirmRequired: true because the assistant should ask “Do you want to place the order now?” before executing it. They also use idempotency keys on the backend to ensure that if the assistant tries the same action twice (due to a retry or error), it doesn’t duplicate an order.

Booking Tools (booking.ts): If the site offers scheduling/booking (like a salon or clinic might), tools such as searchSlots(resource, dateRange) to find available time slots, holdSlot(slotId, customerInfo) to provisionally hold a slot, bookSlot(slotId, paymentToken) to confirm a booking (likely confirmRequired), and cancelBooking(bookingId).

Form Tools (forms.ts): Generic form interaction tools, fillField(selector, value) to fill a specific input field on a form, and submitForm(formSelector) to submit it. These are tied to actual form elements on the page via CSS selectors and are used when the assistant is helping the user fill out a form by voice (e.g., “I’ll fill in your name here... now submit”).

Site Operations Tools (siteops.ts): More behind-the-scenes tools mostly for the agent’s own use, like readSitemap() which can fetch the latest sitemap if needed, respectRobots() to quickly check if something is disallowed by robots.txt (mostly informational), or warmupCache(urls[]) which could hint the client to prefetch certain pages. These are not directly invoked in response to user commands but rather to help the agent plan or improve performance (and they typically don’t directly involve the user).

Additionally, the system supports dynamically generated tools based on the site’s content. This is where the Action Manifest from the publishing step comes in. For each custom action defined in actions.json, a corresponding tool is surfaced to the agent. For example, if the builder defines an action called newsletter.subscribe (triggered by submitting a newsletter signup form), there might be a tool subscribeNewsletter(email) that the agent can call. The parameter schema (email in this case) would come from the form field definitions. These dynamic tools are loaded at runtime for each site by reading the manifest and constructing Zod schemas for each action’s parameters (the manifest may already include a JSON Schema for them). They are then registered into the Tool Registry under that site’s context. In effect, every interactive feature of the site becomes a potential tool the assistant can use. This approach is powerful: as site builders add more custom actions via the no-code interface, the agent automatically gains new “skills” corresponding to those actions.

The Tool Registry ensures that tools are only enabled if the site supports them and the user has permission. For example, on a site with no e-commerce, the addToCart tool might be disabled or not present. Tools can also be toggled by tenant policy (some site owners might not want the assistant to, say, navigate away automatically, so they could disable goto optimistic execution – though by default we allow it). The registry enforces such policies at call time too.

LangGraph Orchestration: The orchestrator logic is implemented with LangGraph, which allows us to define a directed graph of steps (nodes can be LLM calls, tool executions, conditionals, etc.). The overall conversation turn processing might look like this in a simplified graph form:

Input Node: Take the final ASR text and any context (like which page the user is currently on, or the conversation history).

Intent Classification (optional): If an Intent Engine is enabled (as noted in the roadmap), we might first classify the user’s intent into broad categories: e.g., Question, Navigation Request, Transaction (purchase/booking), etc., or even a specific known command. This could be a lightweight model or rules. If it confidently matches a direct action (“navigate to X page”), we could shortcut directly to a tool call.

Retrieval Step: For queries that are informational (or have an information component), we perform a knowledge base search (RAG retrieval). This is often done by calling the siteSearch tool with the user’s query (or relevant keywords) and retrieving, say, the top 5 chunks from the KB. These chunks come with text and metadata (like source URL, etc.). We then prepare a prompt context for the LLM that includes these snippets. We may also include a summary of the site’s action capabilities if relevant (so the LLM knows what it can do).

LLM Decision Step (Planner): We prompt the LLM (GPT-4) with the user’s question, the retrieved context, and a list of available tools (with their schemas and descriptions). The prompt might be something like: “User asked: 'Do you have size M in stock?'. You have knowledge: [Product X is available in sizes S, M, L]. You have tools: addToCart, listVariants, etc. Decide either to answer or use a tool.” The LLM is instructed that it can output either a direct answer or a function call (tool invocation) as per OpenAI function calling protocol. If the query requires an action (like navigating, adding to cart, etc.), or further info via a tool, the LLM will output a function name and arguments.

Tool Execution Step: If the LLM chose to call a tool, the orchestrator takes that output, parses it, and executes the corresponding tool function with the provided arguments. For example, if it called listVariants with a productId, the orchestrator runs the code for listVariants (which might query the site’s product database or the KB) and gets a result (say, a list of available variants with stock counts). We then feed this result back into the LLM (this is like function call continuation in OpenAI API: the LLM gets the function’s output and can continue generating a final answer).

Response Generation: With all necessary info gathered (including any tool results), the LLM now produces the final answer to the user. This answer might include descriptive text and possibly a prompt for confirmation if an action is pending. For instance, after using listVariants, it might answer: “Yes, size M is in stock. I can add it to your cart if you’d like. Should I do that?” – effectively asking for user confirmation.

Confirmation Loop (if needed): If the agent’s plan involves a side-effecting action that was marked confirmRequired, the agent will not execute it immediately. Instead, it will prompt the user (as above). The conversation then waits for the user’s explicit yes/no (or a cancellation). If the user confirms, the orchestrator will proceed to execute the action tool (e.g., addToCart) and then perhaps follow up with the next step (e.g., “Item added to cart, shall we proceed to checkout?”). This mechanism ensures the user is always in control of irreversible or sensitive operations.

Streaming and Optimistic Actions: Throughout this, we leverage the streaming nature. The LLM is capable of returning a partial answer token by token. The moment it starts emitting the final answer, we wrap that in a streaming response to the client, so TTS can begin. If the LLM’s answer includes a decision to navigate or some immediate action, we often already executed that (especially navigations) by this point. For example, if the user said “Go to pricing and what’s the cost of the premium plan?”, the assistant might concurrently navigate to the pricing page (fulfilling the first part) and answer the second part by looking at the pricing info. The navigation (goto) tool would have been called as soon as the intent was recognized, without waiting for the whole answer. This parallelism and optimistic execution greatly improves perceived speed.

All these steps are orchestrated via LangGraph which allows branching and looping. For instance, if the user says “scroll down” during an answer, that interrupts and triggers a navigation tool mid-flight. Or if the LLM is not confident and asks a clarifying question, the graph can loop back to get the user’s answer. We also maintain a short dialogue memory – recent Q&A pairs – so the assistant can use context from the last few turns when needed (e.g., pronoun resolution like “Can you check that for me?” referring to something from the previous turn).

Integration with Site Contract & Tools: The orchestrator is initialized with the site-specific context: it loads the Tool Registry for the site (including dynamic tools from the Action Manifest) and has access to the structured data. One can think of it as having a live API to the website. This is what enables actions beyond pure Q&A. It’s not just answering from a knowledge base, but can do things like click a button, fill a form, retrieve live data. For example, on a restaurant site, if a user asks “Can you book a table for 2 tomorrow at 7pm?”, the agent might: search availability (perhaps call searchSlots tool), see a slot is available, then respond “I found a table at 7pm. Shall I book it for you?” If user says yes, it calls bookSlot. This entire flow is handled seamlessly by orchestrating the appropriate tool calls. The assistant’s reply will include confirmations like that to ensure nothing happens without user consent in transactions.

Because tools are defined with side effect metadata, the orchestrator also logs tool usage and outcomes. Every tool call produces an event (for analytics and debugging) that includes how long it took, what it returned, and if it succeeded or failed. If a tool fails (e.g., a network error or an out-of-stock condition), the orchestrator can catch that and have the LLM produce an apology or alternative suggestion. This is part of error handling in the agent loop.

Agent Graph Example: To illustrate, consider a user on an e-commerce site who says: "I'm looking for a red dress in medium under $50."

The agent classifies this as a product search query.

It calls the siteSearch tool with query "red dress medium under $50".

The tool perhaps uses vector search + filters to find relevant products and returns a list of matching items (title, snippet, URL, price).

The LLM sees the results and decides to respond with a short list: "We have a Red Summer Dress for $45 and a Ruby Cocktail Dress for $49." It also knows it can help further, so it might continue: "Would you like to see one of these?"

Behind the scenes, it might attach an action suggestion (like it knows the URLs, so maybe it can navigate if user says yes).

If user says "Show me the Ruby one," the agent will call the goto tool for that product’s page and perhaps describe it after navigating.

This is a multi-turn interaction orchestrated by the graph, involving search and navigation tools.

LangChain and Function Calling: We leverage OpenAI’s function calling interface to minimize hallucination and maintain determinism. The LLM is given a structured schema for tools, so if it tries to call a tool, we ensure it produces a valid JSON per that schema. We then strictly execute what is specified. This prevents the AI from doing anything undefined or accessing tools it shouldn’t. It can only call the functions we’ve registered and only with allowed parameter formats. If it tries to produce something else, our system will either reject it or treat the output as a normal message.

The agent also has some guardrails: for instance, if a user asks something outside the site’s scope (like a general knowledge question or something inappropriate), the assistant will gracefully decline or deflect (possibly saying it can’t help with that). This is part of prompt conditioning and safety – we include instructions that the assistant should only use knowledge from the site and not reveal anything about the underlying system or any other site (to avoid data leakage). The knowledge base isolation helps here too; it won’t find info from elsewhere.

Logging and Tracing: Each turn with its tool calls is logged (for our monitoring and improvement). We keep transcripts of the conversation (only for the site owner’s analysis and improving responses, respecting privacy settings) and a trace of tool calls (for debugging). This is immensely helpful in tuning the agent’s prompts and behavior over time. It also feeds into the analytics pipeline (so a site owner can see, for example, “most common voice questions” or “how many add-to-cart actions were done via voice”).

In summary, the AI orchestrator is a goal-driven conversational engine that combines LLM intelligence with deterministic tools. It uses the LangGraph framework to manage multi-step dialogues and ensure the workflow (like asking confirmation, handling tool outputs) is robust and traceable. By having the Action Manifest and site data, the agent effectively knows the site’s “API” and can act on it. The orchestrator’s design enables optimistic execution (performing obvious actions immediately) and maintains a high level of trust – it asks the user when it’s about to do something significant (e.g., charge a credit card). The combination of retrieval-augmented generation (for knowledge) and tool usage (for actions) is what makes SiteSpeak’s assistant truly agentic and not just a Q&A bot. It doesn’t just chat about the site; it operates the site on the user’s behalf in a safe, controlled manner. The use of a structured approach with LangChain/LangGraph also means the solution is maintainable and extensible: new tools can be added (say the site owner adds a **“track order” feature – we could add a trackOrder tool, and the assistant would use it when relevant) without retraining an AI model, simply by updating the manifest and registry. This aligns with the no-code philosophy – the AI capabilities scale with the site’s capabilities automatically.

API Gateway and Multi-Tenant API Surface

All client interactions (aside from serving the site content itself) are funneled through a secure API Gateway layer. This gateway exposes unified endpoints for the voice assistant and knowledge base services and enforces authentication, rate limiting, and versioning rules. By having a single gateway, we present a consistent and small attack surface and simplify client configurations (the builder UI and the voice widget both communicate with the same base URL, e.g., api.sitespeak.com/v1/...).

Endpoint Structure: The API gateway is a lightweight Node/Express service (or a Fastify service) under the hood, configured with routes for each functional area. All endpoints are prefixed with an API version, currently /api/v1. The use of explicit versioning means we can evolve the API without breaking existing integrations – a future overhaul would go to /api/v2, etc., following semantic versioning for the HTTP interface. The gateway’s responsibility is mainly to validate requests, enforce policies, and proxy/route the request to the appropriate internal service (which might even be within the same process in our monolith, but logically separated). Importantly, it also streams data for real-time endpoints.

Key endpoints include:

Health Checks:

GET /api/v1/health – a quick check that returns 200 if the service is up (and possibly some info like version). This is used both for load balancer health checks and by our deployment scripts.

GET /api/v1/health/live and GET /api/v1/health/ready – more specific Kubernetes-style liveness and readiness probes. The liveness check just returns OK if the process is alive (not hung), and the readiness check does a deeper dependency check (ensuring that the database, Redis, and external AI APIs are reachable). We ensure these respond very quickly (<10ms ideally) since they may be called frequently by orchestration.

Voice Session Management:

POST /api/v1/voice/session – this endpoint is called by the voice widget when it initializes or when it needs to start a new conversation session. It authenticates the site/user and returns a short-lived voice session token or ID that will be used for the streaming connection. The response might include configuration like which language model to use or what locale the TTS will speak (e.g., it might return {"sessionId": "...", "ttsLocale": "en-US", "sttLocale": "en-US", "expiresIn": 900}). We often implement this as returning a JWT specific to the voice session, which the client will then use to connect to the WebSocket (so the WS upgrade can be authorized).

Voice Streaming Endpoint:

GET /api/v1/voice/stream – this could be used as an SSE endpoint (long-running HTTP connection that the server uses to push events) or upgraded to a WebSocket (e.g., ws://.../voice/stream) for full bidirectional use. In practice, we prefer WebSocket for voice because it allows the audio to be sent up concurrently with events coming down. The gateway handles the protocol upgrade: it checks the Authorization header or session token, and then hands off the connection to the voice service module which manages the WebSocket frames. We ensure that if a connection drops or the client reconnects, there are ways to resume or handle it gracefully (for SSE, Last-Event-ID headers can be used to resume a stream).

Knowledge Base Query Endpoint:

POST /api/v1/kb/search – this endpoint allows querying the knowledge base index for a site. It expects a JSON body with at least a query string. The client might call this if implementing a text chat or search bar (outside of voice) or if the voice assistant wants to retrieve something via HTTP. It returns a list of matches (snippets of text, with maybe titles, URLs, relevance scores). There are optional parameters to filter by tags or content type, limit the number of results, or hint the language. This essentially exposes our vector search + keyword search in a simple API form. Latency is low (a few tens of milliseconds) and we target p95 under 50ms for a search after the index is warm.

Knowledge Base Management:

POST /api/v1/kb/reindex – a privileged endpoint to force a reindexing of the site. Only site owners or internal services can call this (requires auth with the appropriate role). It triggers an async job to recrawl the site. The response is typically just an acknowledgement (202 Accepted) since the work is done in background. The idempotence key here is the site – we ensure only one reindex per site runs at a time (if you call it twice, the second is ignored or queued).

GET /api/v1/kb/status – returns status info about the indexing: last indexed time, number of pages indexed, etc., so owners can see if things are up to date.

Analytics Ingestion: (Likely under /api/v1/analytics/…) – there could be endpoints like POST /api/v1/analytics/events for sending batched events (used by the client to send interaction events, or by the system itself). These would accept data (with some schema validation) and return a result indicating accepted vs dropped events. This is more for internal use (the voice widget might send an event here, e.g., “voice_convo_started”).

User Management and Misc: If the SiteSpeak platform has user accounts or other features (like a login for the site builder or content management), those APIs would also hang off /api/v1/... (for example /api/v1/auth/login, /api/v1/site for CRUD on site definitions, etc.). However, those are more platform admin APIs rather than end-user runtime, so they are not the focus here. We primarily expose runtime APIs for the assistant and content.

Authentication and Authorization: The gateway requires all requests (except truly public ones like health checks or maybe a public search) to include an Authorization header with a Bearer token (JWT) or a session cookie that maps to one. This JWT is issued by our auth system when a user (site owner or a site visitor, depending on context) logs in or is given access. For end-user interactions (like voice widget usage), the JWT might be a short-lived token scoped to the specific site and an anonymous user session. For the builder and admin APIs, the JWT would represent the site owner’s login with appropriate roles.

The gateway verifies the JWT on each request (using HMAC or RSA depending on how tokens are signed). It checks that the token is valid (not expired) and pulls the claims like tenantId, siteId, role. We then enforce RBAC: each route is annotated with required roles (or we infer from context). For example, kb/reindex can only be called by a token where role = owner of that site. The voice streaming endpoint might allow an anonymous or widget role token that is scoped to read-only operations on that site. All tokens carry a tenantId claim and often a siteId, and our middleware will ensure that any path parameter or body specifying a site corresponds to the token’s site (no one can ask data from another site). This tenant isolation is critical for security – it’s implemented at multiple levels (JWT claims, code checks, and even database row policies as a safety net).

For browser clients (the voice widget on a site), we have an additional convenience: if the user has no token (anonymous), the widget may initiate a session by calling an endpoint with the site’s public API key or some one-time token. That endpoint could set an HttpOnly cookie with a session JWT for the domain (e.g., a cookie scoped to *.sitespeakusercontent.com where widget iframes might live, or just used in requests). The gateway supports this by also checking for a session cookie if the Authorization header is not present. All such cookies are set with Secure and SameSite attributes to prevent misuse. We also consider CSRF: for now, most endpoints are read-only or idempotent (except maybe form submissions via tool, which go through the assistant anyway), but if we had state-changing endpoints accessible via browser, we’d include anti-CSRF tokens.

Rate Limiting: The gateway implements rate limiting to prevent abuse and ensure fair usage across tenants. This is done via middleware (for example, using a token-bucket or sliding window algorithm, backed by Redis). We apply limits per API key or tenant and per IP. The limits might be something like “X requests per minute per IP” for public endpoints, and separate “Y requests per minute per site” for expensive endpoints (like search or voice sessions). When a limit is exceeded, the gateway returns HTTP 429 Too Many Requests, along with a Retry-After header indicating how long to wait. We also plan to include the newer RateLimit response headers (draft standard) like RateLimit-Limit, RateLimit-Remaining, etc., so well-behaved clients can adjust their request rate accordingly. The rate limit policies are tuned to avoid impacting normal usage (they’re more as a safety net against spam or accidental infinite loops). We take into account the bursty nature of voice – e.g., the widget might make several requests quickly when a session starts (one for session, then a stream), which we allow. Our rate limiter uses the tenantId and IP as keys, and is careful to distinguish different types of traffic (control vs streaming).

Input Validation and Error Handling: The gateway performs input validation on all API requests. We use Zod or similar schema validators on the request body and query params for each route. This ensures that the format is correct – if not, we return a 400 Bad Request with a standardized error body. We adopt the RFC 9457 Problem Details JSON format for errors throughout. That means the client will get a response like: {"type": "/errors/invalid-input", "title": "Invalid request", "status": 400, "detail": "query field is required", "instance": "...", ...}. Consistent error formatting helps both our front-end and third-party developers (if we expose APIs publicly beyond the widget) to handle issues. The gateway catches any exceptions or rejections from downstream and maps them to appropriate HTTP errors. We make sure not to leak internals; e.g., if a database error occurs, we might return a 500 with a generic message and a logged correlation ID, rather than the raw SQL error.

Every response, success or error, includes a unique correlation ID (we generate one per request at the gateway if the client didn’t send one) in a header like X-Correlation-ID. This ID is also logged in our logs/traces, enabling us to trace a specific transaction across systems (helpful for debugging an issue reported by a user).

Streaming Data Handling: The /voice/stream endpoint is special in that it keeps the connection open. We implement it in a way that can handle dropped connections gracefully. For SSE, the Last-Event-ID header mentioned before allows resume – we tag each event with an incremental ID. For WebSocket, if a connection drops mid-conversation, the client will automatically try to reconnect and establish a new session (perhaps via a new voice/session call). The backend treats each connection fresh, but if needed we could cache some recent conversation state keyed by session ID to allow resuming without starting from scratch. In the current design, if a voice session drops, the user would typically just start a new conversation; maintaining long conversation context after interruption is a future enhancement.

Observability in Gateway: The gateway is instrumented for logging and metrics. It logs each request’s essential info (method, path, user/tenant, response time, status code, correlation ID). It also exposes a /metrics endpoint (authenticated or internal) that Prometheus can scrape, containing metrics like request counts and latencies, broken down by endpoint and result. The gateway attaches a trace/span context to each request (via OpenTelemetry), so that all downstream operations (DB queries, etc.) can be correlated under the same trace. This gives us end-to-end visibility: for instance, a voice query trace might show the HTTP request, then the WS events, then the AI calls, etc., in one timeline.

Security at Gateway: The gateway is the front door, so we implement a number of security measures here:

It only listens on HTTPS (in production) and we enforce TLS 1.2+.

We set appropriate security headers on responses (e.g., Strict-Transport-Security for HSTS, maybe a Content Security Policy to control what the widget can load, etc.).

We limit payload sizes – requests with huge bodies are rejected with 413 Payload Too Large. For instance, kb/search query JSON must be under some KB size. This prevents memory DoS via massive request bodies.

We validate content types (only accept application/json on JSON endpoints, etc.).

We have protections in place for common web vulnerabilities: no reflected XSS (we don’t typically reflect input in responses except in our JSON which is safe), no open redirects (there are none in our API), and so on. The gateway is relatively simple in terms of logic, which helps minimize issues.

The rate limiting and auth, as mentioned, address OWASP API security issues like excessive requests and broken auth. In fact, strict auth on all endpoints (except health) means there’s no such thing as an “insecure direct object reference” – the JWT’s site scope must match any resource ID or it’s a 403 automatically.

We log all auth failures and unusual conditions for monitoring. If we detect suspicious patterns (e.g., a single IP triggering lots of 401 or 429 responses), that can be an alert for potential scanning or abuse.

Success Criteria and SLA: We set certain SLOs at the gateway: for example, p95 latency for non-streaming requests (like kb/search) should be <50ms (after warmup). The voice streaming setup (session creation) should be very fast as well, and the first event on the stream ideally within that ~300ms window as discussed. The gateway itself should never be a bottleneck; it’s designed to be stateless and horizontally scalable. It can be deployed in multiple replicas behind a load balancer, with sticky sessions not required (the voice WS can connect to any instance and we manage state in Redis or in the JWT itself). We also ensure 100% of sensitive routes require auth – an internal security review checklist item (no endpoint that returns user or site data should be public).

In summary, the API Gateway provides a secure, versioned, and monitored interface to SiteSpeak’s capabilities. It encapsulates the complexity of the underlying services behind a clean REST/WS API. By handling auth, rate limiting, and input validation, it frees the core services to focus on business logic. For the user or developer integrating, the API feels simple: e.g., one call to start a voice session, then a single endpoint for streaming that covers the whole conversation, and a call for search or reindex when needed. This aligns with our philosophy of offering SiteSpeak’s features not just through our own UI but potentially as APIs that others (partners or advanced users) could integrate into their own apps, all while maintaining tenancy isolation and security at the forefront.

Security, Privacy, and Compliance

Security and privacy are foundational in SiteSpeak’s design. We deal with potentially sensitive data (website content, user queries including personal info spoken aloud, etc.) and we execute actions (some with financial or personal implications, like making a purchase or booking). Therefore, robust security controls and privacy safeguards are in place at every layer.

Authentication & Session Security: SiteSpeak uses JWTs for authentication between clients (builder UI, voice widget, etc.) and the backend. These JWTs are signed with a strong secret (or asymmetric keys) and have a short expiration (for end-user tokens, often 15 minutes to an hour, with refresh if needed). We enforce the OAuth2 Bearer token standard: the token must be present in the Authorization header for API calls, and we do not allow tokens in URL params to avoid leakage in logs. For the voice widget, which often operates as an embedded script on a customer’s site, we issue a JWT scoped to that site (the site’s public key or ID is used to request one). All JWTs contain a tenantId and siteId claim and typically a role claim. Roles include owner/admin (full control of that site/tenant), editor (maybe content editors with limited permissions), and visitor (for end-user interactions, often an anonymous role). The gateway and internal services check these roles on every action.

We also support session cookies for web apps (the builder UI, etc.), but those are HttpOnly and Secure, and they essentially carry a token reference. If a browser session is authenticated, the cookie approach prevents XSS from stealing the token (since HttpOnly prevents JavaScript access). We implement CSRF protection for any state-changing requests if cookies are used (via either SameSite settings or an anti-CSRF token pattern). However, most interactions from the voice widget use direct token auth rather than cookies, due to cross-origin considerations.

Role-Based Access Control (RBAC): Every API route and internal function checks the user’s role and site scope before proceeding. For example, only an admin or the specific site’s owner can publish or reindex that site. The voice widget token (visitor role) might only allow calling /voice/stream and /kb/search for its site, and nothing else. We map out permissions such that even if a client tried to misuse a token, the worst they could do is what that token is allowed (principle of least privilege). Internally, our services (like the knowledge base and tool execution) also carry the user or system identity through, so if a tool tries to do something not permitted (like accessing another site’s data), it’s denied at that layer too. For instance, the DB layer might enforce RLS policies: a query for knowledge base chunks must include a tenant/site filter that matches the context, otherwise it returns nothing.

Tenant Isolation: As noted, tenant isolation is enforced in multiple ways:

Logical Data Separation: Each tenant (which could correspond to a company or account; in many cases one tenant = one site, but if an account has multiple sites, they share a tenant ID) has their data labeled. All database tables that store tenant-specific data have a tenant_id (and often site_id) column, which is indexed and used in every query’s WHERE clause. We implement Postgres Row-Level Security policies so that if for some reason a query is made from a context that doesn’t have the right tenant ID, it fails or returns 0 rows by default. In code, our repository/ORM layer automatically injects the tenant context.

Memory/Cache Separation: Keys in Redis or in-memory caches are namespaced per tenant. E.g., cached vectors or rate limiter counters carry tenant/site identifiers. There’s no global key that could accidentally mix data. We also never store user-specific secrets or content in client-side objects that could be shared across sites.

No Cross-Domain Access in Frontend: The voice widget is typically loaded in a context where it can only access its own site’s content (it might be an iframe or an embedded script that we ensure only pulls data for that site). We configure CORS such that a widget from site A cannot point to site B’s data via our APIs – the backend will reject it due to the token mismatch and also CORS policy only allowing the specific allowed origin.

Testing: We rigorously test scenarios to ensure no cross-tenant data appears where it shouldn’t. This includes simulating parallel conversations on different sites and verifying logs/answers don’t leak data. We also do security testing (like injecting an ID of another tenant in an API call) to confirm it’s properly forbidden (we expect 403 Forbidden or similar each time).

Secrets Management: The platform interacts with external AI APIs (OpenAI). We treat those API keys as highly sensitive. They are stored only on the server, as environment variables or in a secure vault – never in client-side code. The client (voice widget) does not have direct access to the OpenAI API; it always goes through our server. This prevents abuse of our API key and also allows us to attach the request to a tenant for monitoring and metering usage. Internal secrets (like JWT signing keys, database passwords, encryption keys for sensitive data) are likewise kept out of the code repository and only provided via secure deployment pipelines (Kubernetes secrets or cloud secret managers). In development, we have an .env file (not checked into git) for these.

Encryption: We enforce TLS for all network communication. Data at rest in the database is encrypted via disk encryption (depending on hosting, e.g., AWS RDS encrypted instances). Additionally, for any particularly sensitive user data that might be stored (if any – for example, if the site has user accounts and they go through our system, passwords would be hashed with bcrypt; if credit card info for bookings, we likely integrate with an external PCI-compliant service so we never see raw card numbers). We provided an ENCRYPTION_KEY in config for potentially encrypting fields at the application level (for instance, if we stored OAuth refresh tokens or API tokens on behalf of the user, we’d encrypt those with this key in the DB). This key is 32 bytes for use in AES-256. Fields like user.email might be encrypted such that even if the DB is compromised, PII is not plain. We also sign certain data structures (like the Action Manifest might be signed or hashed to detect tampering, though since it’s generated and used internally, this is less of a concern).

Privacy Controls: By design, the voice assistant does not record or store voice data unless needed. The audio that comes in is streamed to ASR and not saved on disk (unless the site owner explicitly enables a feature to store transcripts or recordings for analytics/improvement, which would be an opt-in). Even then, we default to not storing raw audio – we’d store transcripts (which are less sensitive than someone’s voice print, though still personal data). We also ensure that any personally identifying information (PII) in content is handled carefully. Our knowledge base transformers include a PII scrubber that can redact things like phone numbers or emails from being stored in the vector index if they appear in user-generated content, to avoid inadvertent exposure. For analytics events, as mentioned, we avoid collecting things like full IP addresses (geo info only) and we hash or pseudonymize user identifiers.

GDPR/CCPA Compliance: We treat the end-customer’s website data as their data, and the voice interaction logs similarly. We have in place the ability to delete a site’s data on request (right to erasure) – meaning remove their knowledge base entries, analytics, etc. If a user of a site (an end-user) requested their conversation transcript to be deleted, and if those are stored, we could locate and delete them as well (this would be rare and would likely be handled by the site owner through a tool we provide). We also can export conversation logs associated with a particular user ID if needed for data access requests, though since most voice interactions are anonymous (no login), we might not have a way to identify a specific person’s sessions unless they identify themselves in conversation.

We have a consent mechanism for analytics cookies or any tracking. The voice assistant by default might operate under necessary cookies (no tracking beyond functional events). If more detailed tracking is enabled (like recording interactions for improving the AI), the site is expected to disclose this in their privacy policy (we assist by providing template language). We ensure our use of personal data is within the scope given – e.g., voice transcripts are used to generate responses and optionally to improve the system (we might retrain language models or adapt prompts using aggregated transcripts, but that’s done in a way that doesn’t tie back to individuals and is allowed under our terms).

Compliance Standards: We align with OWASP ASVS (Application Security Verification Standard) and OWASP Top 10 best practices. For instance, ASVS requires things like enforce strong authentication, no default credentials, safe memory management, etc. Concretely, we do things like:

Require a minimum length and complexity for passwords (for site owners’ accounts).

Implement account lockout or throttling on login attempts to prevent brute force.

Use HTTPS only (HSTS as mentioned).

All third-party libraries are monitored for vulnerabilities (we run npm audit and have Dependabot etc., plus part of our CI pipeline fails on high-severity vulns).

We have a content security policy (CSP) that by default only allows scripts from our domains on the voice widget frame, mitigating XSS risks for the widget integration.

We have checked that the voice injection script cannot be used as an XSS vector on the host site – it runs in an isolated scope or iframe so it doesn’t expose site cookies or DOM to other sites.

Sensitive data like API keys are never exposed to the client or stored in logs.

The JWTs are properly validated using secure libraries (preventing attacks like algorithm confusion, replay attacks via jti+exp checks, etc.). We rotate secrets if needed and can revoke tokens (short expiry helps in that regard too).

We protect against common attacks: e.g., SQL injection is prevented by using an ORM and parameterized queries (and we double-check any raw queries). Command injection isn’t as relevant in our stack, but we avoid dynamically calling eval or executing shell commands with user input. No eval in the codebase except perhaps in a controlled sandbox for the agent (the agent might have a pseudo code executor, but that doesn’t run on our server without controls).

Rate limiting and input validation cover DoS and overflow attacks (we won't parse extremely large JSON input, etc.).

Continuous Monitoring: We have tooling to monitor the security of the production deployment. This includes:

Logging in a SIEM where we set up alerts for abnormal patterns (like a single IP scanning lots of site IDs in API calls resulting in many 403s, which could indicate someone trying to guess URLs).

Web application firewall (WAF) rules in front of the gateway for generic threats (AWS or Cloudflare WAF can block known malicious patterns).

Regular penetration tests or using automated scanners against our API (with caution not to trigger false positives in the AI).

A responsible disclosure policy and perhaps bug bounty for external researchers.

User Data and Compliance: Since SiteSpeak can be integrated by businesses that have their own users, we position ourselves as a processor of data under GDPR (with the site owner as the controller). We provide features for them to comply with user requests as mentioned. We also ensure that if the end-user says something sensitive in voice, those voice transcripts are protected as described. If any location or special category data were processed, we would handle it with additional care (though likely not applicable unless user shares it in conversation, which is freeform).

Auditing and Logs: All admin actions (like a site owner triggering a reindex or using the builder to change something) are logged with who did what and when. This audit log can be surfaced if needed to investigate incidents. Likewise, internal errors or unusual agent decisions are logged for developers to review. We have an “AI trace” log for each voice conversation (especially during development) that shows each tool invoked and any policy triggers (like “confirmation required” events). This is useful not just for debugging but also to ensure the agent followed the rules (if it ever attempted to do something it shouldn’t, we’d catch it in logs and can adjust prompts or code accordingly).

In summary, security is layered throughout: from the perimeter (gateway enforcing auth/rate limits), to the data layer (tenant isolation, encryption), to the application logic (confirmations for actions, no unauthorized operations). Privacy considerations shape what we log and store (favoring ephemeral processing). We align with industry standards and legal requirements to protect both our clients (site owners) and their users. By doing so, we build trust that using SiteSpeak’s voice features won’t introduce vulnerabilities or compliance headaches to their platform.

Observability and Monitoring

Operating a complex platform like SiteSpeak in production requires robust observability – we need to monitor system health, performance, usage trends, and quickly diagnose issues. We have built comprehensive logging, monitoring, and alerting into the platform, leveraging both open-source tools (like OpenTelemetry, Prometheus) and best practices for high reliability.

Structured Logging and Tracing: Each service in SiteSpeak emits structured logs (JSON logs) with consistent fields such as timestamp, level, service name, correlation ID, tenant/site, user (if applicable), and a message or event description. For example, when a voice session starts, the voice service logs an INFO with the site and a new session ID. When a tool is executed, it logs what tool, what arguments (sanitized), duration, and outcome (success/failure). Errors are logged with stack traces and marked as ERROR with the correlation ID so they can be traced back.

We instrumented the code with OpenTelemetry (OTel) for distributed tracing. There is an Otel middleware on the API gateway that starts a trace span for each incoming request (the correlation ID we generate is used as the trace ID or part of it). This trace context is propagated to downstream calls – for instance, the gateway calls into the voice service module; a span is created for that. If the voice service calls the knowledge base (e.g., a DB query), that is captured as a child span (we use OTel’s Postgres instrumentation to time queries). Similarly, calls to external APIs (OpenAI) are wrapped in spans. As a result, we can visualize a single user interaction across the system: e.g., “User asked question” trace might show -> HTTP request in gateway (10ms) -> voice service processing (span) -> vector DB query (15ms) -> LLM API call (200ms) -> etc., culminating in the response sent. We output these traces to our logging system and also export to a tracing backend (like Jaeger or a cloud APM service) for deep analysis. This helps greatly in debugging performance issues or errors – we can see exactly where time was spent or which component threw an error.

Metrics (Prometheus & Business KPIs): We have a Prometheus endpoint (/metrics) on each service that exposes low-level and high-level metrics. Low-level metrics include process stats (memory usage, open file descriptors), Node.js event loop lag, garbage collection pauses, etc., as well as HTTP request counts/latencies by route (via prom-client middleware). High-level metrics (sometimes called “golden signals”) include:

Request latency distribution for key endpoints (e.g., p50, p90, p99 for /voice/stream handshake, for /kb/search queries, etc.).

Throughput: number of voice sessions started per minute, number of messages processed, etc.

Error rates: rate of 5xx responses, rate of ASR failures, tool error counts.

Resource saturation: CPU usage, memory usage, and also application-specific like how many active WebSocket connections right now, how many DB connections in pool, how long is the job queue length, etc.

We also define business metrics and funnel metrics that are computed and stored by the analytics service. For example:

Voice SLA metrics: We measure the actual timing of voice interactions: “first response time” (from user stop speaking to assistant start speaking) and “speech latency” percentiles. These are tracked and aggregated in our analytics DB so we can ensure our promise of e.g. 300ms p95 is being met. If it’s not, an alert triggers.

Tool usage metrics: How often each tool is invoked, and success vs failure counts. For instance, we know how many times addToCart was called and how many of those resulted in confirmation needed vs completed. This helps detect if a particular tool or integration is failing (if placeOrder tool fails 50% of time, maybe the payment API is down or misconfigured).

Knowledge base metrics: e.g., “retrieval success rate” – percentage of queries that returned at least one relevant result (we approximate relevance by whether the user’s next turn implied satisfaction or not, or we might periodically do info retrieval evals). Also content freshness: how many pages have not been indexed in more than X days versus their lastmod (to catch if the crawler fell behind).

User engagement metrics: via analytics events we track how many voice interactions lead to certain outcomes (like conversion: “user added to cart via voice” or “user completed a booking via voice”). The funnels we compute – e.g., voice interactions -> product view -> add to cart -> checkout -> purchase – are available to site owners in a dashboard. These are updated in near-real-time by aggregating the events we ingest.

The analytics service (see source-of-truth analytics) stores raw event logs and also builds some materialized views or pre-computed aggregates for fast querying of these metrics. For example, it might maintain a rolling 1-hour and 1-day summary of voice latency percentiles, so we can query “today’s p95” quickly. We align these metrics with standard frameworks where possible (like OpenTelemetry metrics semantics, so they can be exported to other monitoring tools if needed).

Alerting and Dashboarding: We have set up alerts for key conditions:

Latency alerts: if p95 voice response time goes above, say, 500ms for more than 5 minutes, that pages the engineering team. Similarly, if vector search p95 goes above 100ms consistently (maybe indicating DB issues), an alert triggers.

Error alerts: a sudden spike in error rate (5xx responses or unhandled exceptions in logs) triggers an alert. For instance, if any service logs more than X errors in Y time, or if the percentage of error responses exceeds a threshold.

Resource alerts: if CPU or memory usage on any container goes beyond a threshold (e.g., CPU > 80% for a sustained period or memory > 90%), we get alerted – this might indicate the need to scale out or a memory leak.

Integration alerts: if external API calls are failing at high rate (we measure OpenAI API call success; if a certain percentage fail or latency spikes, we notify – sometimes the provider might be having an outage or a network issue).

Security alerts: e.g., if we see a large number of failed auth attempts or rate limit throttles from one source, that could alert DevOps/security.

We use a combination of Prometheus Alertmanager for infrastructure alerts and perhaps integrate with a service like PagerDuty or Slack for notifications. The structured logs with correlation IDs allow on-call engineers to quickly grep the logs for a specific correlation ID or search in our log aggregator (like ELK or CloudWatch) to trace through an incident.

For dashboards, we have:

System Health Dashboard: showing CPU/memory of each service, request rates, error rates, and uptime. Also includes number of active voice sessions right now, and maybe distribution of sessions across tenants (to spot any tenant abusing the system).

Voice Performance Dashboard: showing voice-specific metrics: distribution of ASR latency, TTS latency, overall turn-around time, barge-in count, average user utterance length, etc. This helps us refine the voice subsystem and see the impact of any changes (like if we switch ASR provider, we’ll see the latency changes here).

Usage Dashboard for Site Owners: Each site owner can access analytics in their admin UI: number of voice interactions per day, what users ask most, how many times did the assistant successfully answer vs had to say “I don’t know”, conversion metrics (e.g., “Voice interactions that led to add-to-cart: 10 today”). These are powered by the analytics events and reports we generate. We provide visualizations of funnels (drop-off at each step) and pie charts of tool usage (maybe “voice usage by feature: 50% questions, 30% navigation commands, 20% transactional commands”).

Quality Monitoring Dashboard (internal): We also have internal dashboards to monitor the AI quality: for example, how often are conversations rating good vs bad (if we have a thumbs-up/down feedback mechanism, those stats are tracked). If an assistant frequently says “Sorry, I didn’t get that,” those instances are flagged and counted. This helps in iterative improvements of prompts or adding new FAQs to knowledge bases.

Automated Recovery and Self-Healing: Monitoring isn’t just passive; we incorporate some automated recovery measures:

If the voice service process experiences an uncaught exception (shouldn’t happen often due to our error handling), the process will crash. Kubernetes (or our process manager) will immediately restart it (liveness probes ensure it gets restarted if unresponsive). Because stateless design, recovery is quick and usually invisible to users (they may just have to reconnect their session).

If we deploy new versions, we use rolling updates with readiness checks so that a new instance only starts getting traffic when it’s fully ready (database connected, etc.), and old ones drain existing connections (for WebSocket, we might implement a draining mechanism or short TTL on sessions so they reconnect to new).

The queue system (BullMQ) has retry logic for jobs. If a knowledge base crawl job fails (maybe a transient network error), it automatically retries with exponential backoff. If it fails repeatedly, it goes to a dead-letter queue which we monitor. We have metrics/alerts on DLQ size – if many jobs land there, something’s wrong with the pipeline.

The system also does graceful degradation: For example, if the vector search fails for some reason during a query, the assistant can fall back to maybe keyword search or at least respond, “I’m sorry, I’m having trouble accessing some information.” It won’t just crash. Similarly, if the TTS fails (maybe our provider is down), we could fall back to a secondary TTS provider or output text for the user to read. These contingencies are in place to increase resilience. Such events are logged as warnings (and counted so we know if our fallback is being used often, which would indicate a provider problem).

Testing and CI: We consider monitoring as something that should start in pre-production. We have integration tests and end-to-end tests that run in a staging environment and actually simulate voice conversations. During these, we measure the response times and check logs for errors, essentially performing a subset of monitoring in CI. We also use load testing (with tools like Artillery or JMeter) to simulate many concurrent voice sessions to see that our metrics remain within acceptable range (e.g., latency doesn’t degrade badly at X concurrent sessions). This helped us set our baseline and scaling rules (we might autoscale services based on CPU or queue backlog).

Compliance Monitoring: Because we care about compliance, we also monitor things like access logs for administrative access. For instance, every time someone (including our staff) accesses a site’s conversation logs or knowledge base, that could be logged and reviewed to ensure no unauthorized access. We maintain an audit trail of configuration changes (like if someone changes the privacy setting to store audio, we log who did it and when).

In essence, SiteSpeak’s observability stack ensures we have real-time visibility into the system’s behavior and users’ experience. We can answer questions like: Is the system fast and working now? (check dashboards/alerts), How has it been performing over time? (analytics and metrics history), What happened in this specific user session? (traces and logs with correlation ID), and Are we meeting our product goals? (conversion and usage metrics). This closed-loop of monitoring and improvement allows us to confidently iterate on the platform while maintaining reliability. Our on-call processes and automated alerts catch issues often before site owners notice them. And our analytics give site owners the confidence that the voice assistant is actually delivering value (or insights when it’s not, so they can adjust their content or we adjust the AI). Ultimately, observability is not just about keeping servers running, but ensuring the quality of the voice interactions and the success of end-users’ tasks, which our monitoring strategy fully embraces.

Performance and Scalability

Performance has been a major consideration for every component of SiteSpeak, given the real-time nature of voice interaction and the need to scale to many websites and users. Here we summarize how the system meets our performance targets and how it scales as load grows.

Latency Targets and Achievements: We have strict latency targets for key operations:

Voice turn latency: ≤300 ms from end-of-user-speech to start-of-assistant-speech (first token). In practice, we meet this by overlapping ASR and agent reasoning as described. Measurements show first-word latency often around 200–250 ms in good network conditions, and we monitor to keep it under 300 ms p95. Partial transcripts appear on average within 100–150 ms of words being spoken, which feels instant to users.

Knowledge base query: ≤50 ms for a semantic search (vector + hybrid) on p95. With pgvector and HNSW, we typically get ~10–30 ms response times for top-K queries on indexes of a few thousand vectors (which is typical per site). This scales logarithmically with more content; for a site with, say, 100k chunks, HNSW might return results in ~30 ms. If needed, we can increase ef_search or use approximate settings to balance recall and speed.

Tool execution time: Most tools are quick (a DB lookup or a UI action). We budget ≤400 ms for any single tool execution, and many (navigation, simple searches) complete in tens of milliseconds. If a tool is expected to be slow (like calling an external API), we design it to stream results or at least not block critical paths. For example, a startCheckout tool might initiate a process and immediately return an acknowledgment that we started (while the checkout page loads separately).

Page load performance: The published static sites are optimized to meet Core Web Vitals: Largest Contentful Paint (LCP) under 2.5s, Interaction readiness (INP) under 200ms, etc., for a median user. We achieve this via CDN caching and content optimization (e.g., images responsive, code splitting, etc.). The presence of the voice widget does not significantly impact this – it’s loaded asynchronously. We test sites with and without the widget to ensure it doesn’t degrade performance beyond, say, 50ms of main thread work and a small bundle (we aim for the widget script to be lightweight, possibly loaded from a CDN).

Concurrent capacity: Each voice server instance (pod) can handle on the order of hundreds of concurrent voice streams (WebSocket connections), because the heavy lifting (ASR/TTS) is done by external services. The main load on our server per stream is handling audio packets and forwarding events, plus the LLM call. The LLM calls we serialize per session (we don’t usually run two GPT-4 queries in parallel for one session), so the CPU usage is moderate – GPT is external. We ensure the event loop isn’t blocked: audio processing is minimal (just piping bytes). In testing, we’ve seen that a single Node.js process can handle e.g. 1000 concurrent WebSockets sending 20msg/sec of small frames before saturating one core, which is plenty. We also offload some things to worker threads or separate processes if needed (for example, the speech decoder if we ever ran one locally, but currently we don't – it's cloud).

Throughput for indexing: When onboarding a new large site or doing a full re-index, performance matters but is not as critical as real-time flows. Our crawler can fetch and process perhaps 5–10 pages per second per site (this is variable with page size and render time). With Playwright, rendering could dominate if pages are heavy with JavaScript. We are okay with, say, a 100-page site being indexed in a couple of minutes. Since it’s often asynchronous, that’s acceptable. We can parallelize crawling across sites easily (each site’s job is independent, and we have concurrency controls so we might crawl up to N sites at once, configurable). The vector embedding step is often the slowest (due to calling OpenAI embed API). We batch embedding requests to amortize overhead; OpenAI embedding API can handle up to 2048 tokens per request for example, so we batch multiple chunks. This keeps indexing efficient – e.g., embedding 1000 chunks might take a few seconds. We also consider using multiple embedding workers in parallel if a site is huge.

Scalability Design: SiteSpeak is designed to scale horizontally:

The stateless services (API Gateway, Voice/AI service, Analytics ingest) can all be run with multiple instances behind load balancing. They do not store session state in memory (except the minimal transient info which can be reconstructed from a token). WebSocket connections are typically routed to a specific instance and stay there, but since any instance can handle any site, the distribution of sites across instances is not a problem. If we needed to scale to thousands of concurrent voice sessions, we’d run many voice server pods; the load balancer will distribute new sessions among them roughly evenly (sticky sessions might pin one session to one instance, which is fine).

The database (Postgres) is vertically scalable to a point (we use a strong instance with read replicas if needed). We anticipate the amount of data per site to be modest; even if we host 1000 sites with on average 100 pages each, that’s 100k pages and maybe 1M chunks – which Postgres can handle easily on a single instance with proper indexing. If we onboard significantly more/larger sites, we can partition by tenant or use multiple databases (shard by tenant). Because tenants are isolated, such sharding is straightforward (each site could be assigned to a DB instance). But likely unnecessary until we hit high volume (millions of pages indexed).

Vector search scaling: pgvector with HNSW is quite efficient up to millions of vectors. We also have the option to move to an external vector store like Qdrant or Milvus if needed, but keeping it in Postgres avoids extra moving parts. We have tested with ~1M vectors to ensure query times are still within our limits by tuning HNSW parameters (which they were, ~30-40ms for k=5 on a 1M dataset with proper index parameters).

Caching: We make use of caching at multiple levels to improve performance as we scale. For example, the knowledge base search results for a query might be cached for a short time (especially if an identical query comes repeatedly from different users). We use an in-memory LRU or Redis to cache recent kb_search results keyed by (site, query) for say 1 minute – since site content doesn’t change that fast normally – resulting in sub-millisecond responses on cache hits. Similarly, we cache the OpenAI API responses for embedding identical text (dedup by content hash) so we don’t recompute embeddings if the same exact text already was embedded elsewhere (rare across sites, but could be common boilerplate).

CDN scaling: The static site content is served via CDN, which can handle large volumes of traffic easily. Even if one of our sites goes viral and gets millions of hits, the CDN (e.g., Cloudflare or AWS CloudFront) will cache most content and offload it. Only the voice API calls would come to our servers; those are far less in volume than general page requests.

Autoscaling: We configured our Kubernetes cluster to auto-scale certain deployments based on CPU and memory. For example, the voice service might autoscale if CPU > 70% for 5 minutes. Similarly, the ingestion workers could scale out if there is a backlog of indexing jobs (so if many sites updated at once, we spin up more workers to catch up faster). We also can scale the vector DB by adding read replicas and splitting read requests (though in our design, most writes/reads happen on the primary for consistency – but heavy search load could be directed to replicas if we eventually use a separate search service).

Contention and Locking: We ensure minimal blocking between tenants. Each site’s operations largely happen independently. The only shared resources are things like the embedding API rate limits (OpenAI has per-key rate limits – we manage that by possibly using multiple keys for high volume or requesting rate limit increases, and by queuing requests). Within our DB, we avoid long transactions. The knowledge base writes are upserts per chunk – those are fairly quick, and we commit per page to not lock too much. We also leverage Postgres advisory locks for things like not crawling the same page concurrently. These are lightweight and scoped to site+page. Contention on those would only happen if something went wrong (like two jobs accidentally on same site).

Optimizing Hot Paths: We identified hot code paths and optimized them. For instance, audio handling on the WebSocket is done in raw binary to avoid base64 overhead (some older systems use base64 in JSON, which adds 30% overhead – we avoid that by truly sending binary frames). We choose efficient data formats (Protobuf or JSON as appropriate; for events we stick to JSON since it’s fine, but for audio we go binary). We use Node.js streams efficiently to pipe data to ASR APIs without buffering entire files, etc.

Parallelism in Orchestration: The agent can in some cases parallelize tasks: e.g., retrieve from knowledge base and call an external API concurrently if the prompt allows for it. While GPT function calling is sequential (one at a time), our architecture could allow spawning multiple tool calls in parallel if we implemented that logic outside of GPT. Currently, we don’t heavily parallelize inside one query (to keep logic simpler and because one GPT at a time is usually enough), but the architecture doesn’t preclude it if we had a use-case (like maybe pre-fetching likely needed info as soon as the question is heard).

Scalability Testing: We have performed load tests where we simulate, say, 100 concurrent voice sessions all asking questions continuously. The system handled it with linear scaling (the main constraint was OpenAI API calls, which we mitigate by having a pool of API keys and also caching embeddings). Database load stayed in check (because vector searches are indexed and quick). We plan capacity such that the peak expected concurrency can be handled on our current infrastructure with room to burst.

Zero-Downtime Deployment: Thanks to Kubernetes and health checks, we achieve rolling deployments without downtime. We’ve also built the system such that schema changes (migrations) can be done without breaking running code (when necessary, we use techniques like adding new columns that are optional, deploying code that starts writing to both old and new columns, etc., then removing old in a later deploy – classic blue/green DB migrations). This ensures we can push updates or fixes rapidly (important for a SaaS platform) without interrupting service. Users might not even notice when a new version of the AI logic is rolled out except perhaps improvements in quality.

Global Distribution: If needed, we can deploy regional instances for lower latency. For now, our services run in (for example) a single region (say US East); that yields ~100-200ms extra latency for users far away. Since 300ms is our budget just for model processing, adding 100ms network still keeps things reasonable (~400ms). But for truly global audiences, we might deploy an EU or Asia cluster and route users accordingly (especially relevant if voice is used globally). The design is such that multiple clusters could operate with separate DBs but a shared login system. This isn’t implemented yet but is considered for future scaling.

Content Delivery & Edge Caching: Besides CDN for static pages, we also consider edge caching for parts of the voice interaction. For instance, if certain knowledge base Q&As are very common, we could cache the final answer at edge for a short time. However, because our answers can be dynamic and follow-ups vary, this is less straightforward. We do cache within a session heavily: if the same question is asked twice in a row, the second time we can answer almost immediately (we store the last answer). But cross-session caching is tricky due to personalization or context differences. Instead, we focus on speeding up the computation (which we did by ensuring single-turn answers involve at most one LLM call and one vector search).

Backup and Recovery: While not directly performance, it’s part of reliability. We perform regular backups of our Postgres database (point-in-time WAL archiving, etc.), and our vector index can be rebuilt from scratch if needed because the source text is stored (embedding can always be recomputed). The static site files are on durable storage (and likely also in a git-like history or object storage versioning). If an outage occurs, we have contingency to recover quickly on a new environment (infra-as-code to redeploy, and restore DB backup). This ties into scalability in that we can scale across failure zones (multi-AZ deployment) for high availability. Our components are deployed across multiple availability zones so that even if one data center goes down, the service continues (at most a brief hiccup while connections failover).

Future Growth: We have plans for further optimizations: e.g., using a smaller local model for certain simple tasks to reduce API calls, batch processing multiple users' requests together if feasible, etc. But currently, with GPT-4, we handle each query separately. If usage skyrockets and cost/performance becomes an issue, we might incorporate model distillation or fine-tuning to handle FAQs in a cheaper model and reserve GPT-4 for complex cases. Our architecture (via tool gating and intention detection) can support routing queries to different models based on complexity – that’s an area of scalability (cost scalability) we consider.

In conclusion, SiteSpeak is built to scale out and handle increasing load while maintaining its performance SLAs. We regularly test and measure against our benchmarks (300ms voice response, etc.) and have capacity plans to add resources or optimize when metrics even start approaching limits. This performance-centric approach ensures that as more sites and users come on board, the experience remains fast and seamless, which is crucial for a real-time voice system. Our combination of efficient algorithms (HNSW, streaming protocols), horizontal scaling, and smart caching lays a strong foundation for growth.

Conclusion and System Diagram

SiteSpeak’s architecture is the result of a rigorous integration of modern web engineering, AI orchestration, and distributed systems design. It achieves a unified platform where non-technical users (using the no-code builder) and end-users (conversing with the voice assistant) both benefit from the system’s intelligent, efficient design. Each part of the system – from content ingestion to real-time speech – is built with best-in-class practices and with a clear contract on how it interfaces with other parts.

In summary, every published site is automatically voice-enabled, agent-aware, and indexable:

The Site Contract spec (sitemap, JSON-LD, ARIA, Action Manifest, GraphQL) guarantees that the site exposes its structure and capabilities in a standard way. This is the backbone that connects the no-code builder output to the AI logic.

The knowledge base acts as an always-updated index of the site’s content and functions, using hybrid search (vectors + keywords) and stored in a high-performance pgvector store. It ensures the assistant always has evidence to back its responses and knows what actions are possible.

The voice assistant subsystem provides a snappy, streaming conversation experience with full-duplex audio and responsive feedback, leveraging state-of-the-art STT/TTS and our custom barge-in handling. It turns voice into a viable interface for web navigation and transactions.

The AI orchestrator (powered by LangGraph and an LLM with tool use) gives the assistant the ability to carry out multi-step tasks and invoke site-specific actions safely. It’s essentially the “brain” that uses the knowledge (from KB) and the “hands” (the tools) to satisfy user requests, all while conversing naturally.

The platform infrastructure (API Gateway, security layers, analytics, deployment pipeline) binds everything together in a secure, scalable way. Multi-tenancy is enforced at every step, and operational telemetry is fed back for continuous improvement.

For a high-level view, consider the following system diagram of SiteSpeak:

[ User Browser ]
     |
     | (1) User speaks, audio captured by Voice Widget (Web)
     |--> [Frontend Voice Widget] --(audio frames)--> [Voice WS API] (Node backend)
     |                               (stream via /voice/stream)
     |
     |                +--> [ASR Service (OpenAI)] (3) audio -> text
     |                |         |
     |   (2) WebSocket forwards audio, receives partial transcripts / actions
     |                |         v
     |                +-- [Voice/AI Orchestrator Service]
     |                      - handles dialog state & LLM calls
     |                      - (4) Queries Knowledge Base -> [Postgres+pgvector]
     |                      - (5) Calls Tools (site actions, DB, etc.)
     |                      - (6) Sends response text to TTS
     |                            |
     |                            v
     |                      [TTS Service (OpenAI)] (7) text -> audio
     |                            |
     |               (8) Stream audio and agent messages back down WS
     |<-- (9) Play TTS audio to user, highlight actions on page, show text
     |
     +----> (A) If user clicks or types, also goes to Orchestrator (similar flow)

(Numbers indicate the general flow of a voice query; letters indicate other interactions.)

In parallel to the above runtime flow:

The [Knowledge Base Indexer] continuously runs in the background. When the site is published or updated, (B) the Builder triggers a reindex event. The [Crawler/Indexer] fetches pages (rendering with Playwright if needed), (C) processes content (HTML, JSON-LD) and updates the Postgres+pgvector store. This ensures the Knowledge Base stays fresh with minimal delay.

The [Analytics & Monitoring] component collects events from both voice interactions (every turn, tool use, error) and user behaviors (page views, conversions). These events are validated and stored (D). The system generates real-time metrics and periodic reports from this data, which feed into (E) Dashboards for both our team and site owners (for insight into usage and performance).

The [API Gateway] sits in front of all external APIs (voice, kb search, etc.), ensuring that each request is authenticated (site-specific tokens) and authorized. It’s omitted in the flow diagram above for simplicity, but it is the entry point for (1), (A), etc., vetting the request before handing to the respective service.

From a deployment perspective, all server components (Gateway, Orchestrator, Indexer, etc.) run in a Kubernetes cluster, making it easy to scale, update, and maintain. The static site content is on a CDN, the Postgres DB is managed (with read replicas and PITR backups), and external dependencies like OpenAI services are abstracted behind our proxy with fallback strategies. We have a CI/CD pipeline that runs tests and security checks (like linting for any disallowed patterns, running unit and integration tests, etc.) and then deploys in a staggered manner (e.g., canary one instance, then roll out to all). Observability tools immediately catch any anomaly after deploy, allowing quick rollback if needed – though with our strong test coverage and staging runs, issues in production are rare.

Finally, regarding naming conventions and maintainability: the codebase follows a consistent convention – for example, backend TypeScript classes and types use PascalCase, functions use camelCase, and filenames are kebab-case or camelCase depending on context. Folders are organized by feature (e.g., services/voice/turnManager.ts, services/ai/tools/commerce.ts, etc.), and all cross-cutting utilities live under _shared as described. This structure makes it easy for developers to navigate and for new engineers or AI agents to find the “source of truth” for each logic area. Extensive README and documentation (like the ones we cited) are kept alongside the code, ensuring that this blueprint remains a living document – as features are added, the spec and architecture docs are updated.

In conclusion, SiteSpeak is engineered to bring conversational intelligence to every website without requiring any coding or AI expertise from the site owner. We combined principles from web development, voice UX, and AI tool orchestration to create a platform that is innovative yet robust. It adheres to web standards (ensuring accessibility and SEO), respects privacy and security boundaries, and leverages cutting-edge AI to provide a delightful end-user experience. This proposal and blueprint can guide the implementation and scaling of SiteSpeak from initial development through to a production system serving potentially thousands of voice-enabled websites, all while maintaining a high bar for performance, security, and usability.

Sources: The architectural decisions and specifications above draw on our internal documentation and prototypes, including the SiteSpeak core README, detailed subsystem design docs (API Gateway, Voice UX, Knowledge Base, AI Tools, UI/UX guidelines, Analytics, and Security notes), as well as engineering best practices from OWASP and industry examples. These references (as cited in-line) ensure that the blueprint is grounded in proven ideas and that each claim (from latency numbers to API formats) is backed by our design evidence.
